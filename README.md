# DelphiConsensus ğŸ§   
**Simulating Expert Feedback Loops Using LLMs**

DelphiConsensus is an open-source project that simulates the Delphi Method using Large Language Models (LLMs). It orchestrates multi-round expert feedback from multiple simulated SMEs (Subject Matter Experts), enabling consensus-building through AI. Originally designed for adaptive learning and decision support, the system supports plug-and-play LLMs, customizable personas, and a modular pipeline.

---

## ğŸ” Core Idea

> *â€œWhat if you could ask a panel of experts â€” from psychology, policy, economics, or tech â€” and watch them evolve their opinions through structured dialogue?â€*

This project builds exactly that. DelphiConsensus enables:

- SME simulation across multiple subjects
- Multi-round opinion generation and summarization
- Consensus exploration through disagreement and revision
- Optional document-grounded reasoning via RAG (planned)

---

## ğŸ§± Project Structure

```bash
DelphiConsensus/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm_interface.py        # Connects to LLM backend (HuggingFace / OpenAI)
â”‚   â”œâ”€â”€ simulate_experts.py     # Simulates expert responses for a round
â”‚   â”œâ”€â”€ summarize_round.py      # Generates a summary from round responses
â”‚   â”œâ”€â”€ run_delphi.py           # Orchestrates the Delphi simulation process
â”‚   â””â”€â”€ os_utils.py             # [Placeholder] OS/file system utilities (TBD)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ round1.json             # Responses from first round
â”‚   â”œâ”€â”€ summary1.txt            # LLM-generated summary
â”‚   â”œâ”€â”€ round2.json             # Responses post-summary
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml             # LLM + runtime configuration
â”‚   â””â”€â”€ personas.json           # Defines SME persona behaviors and styles
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ prompt_experiments.ipynb # Exploratory prompt tuning and testing
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # You're here

---

## ğŸ§  How It Works

1. **User provides**:  
   - A question  
   - One or more subjects  
   - (Optional) Knowledge base files for RAG use

2. **System generates SMEs**  
   - Each SME tied to a subject  
   - Prompted via persona and configuration settings

3. **Simulation Loop**  
   - **Round 1**: SMEs provide initial responses  
   - **Summarization**: LLM generates cross-expert synthesis  
   - **Round 2**: SMEs revise opinions based on the summary  
   - Repeatable if desired

4. **Outputs**  
   - Individual responses  
   - Summary  
   - Delta/consensus visualization (planned)